import sqlite3
import json
import os
import requests
from bs4 import BeautifulSoup
import time # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö time.sleep()
from transformers import pipeline

SUMMARIZATION_MODEL = "sshleifer/distilbart-cnn-12-6"

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠ Server ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
try:
    # 'cpu' ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å RAM 8GB
    summarizer = pipeline("summarization", model=SUMMARIZATION_MODEL, device='cpu')
    print(f"‚úÖ AI Summarization model {SUMMARIZATION_MODEL} loaded successfully on CPU.")
except Exception as e:
    print(f"‚ùå ERROR: Failed to load AI model. Summarization will be skipped. Detail: {e}")
    summarizer = None
def get_ai_summary(text_content):
    """‡∏£‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° (content) ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ"""
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏û‡∏≠
    if not summarizer or not text_content or len(text_content) < 50:
        return "Summary not available."
    
    # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏£‡∏±‡∏ö input ‡πÑ‡∏î‡πâ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 1024 tokens ‡∏´‡∏£‡∏∑‡∏≠ ~4000 ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞)
    content_to_summarize = text_content[:4000]

    try:
        summary = summarizer(
            content_to_summarize, 
            max_length=200,  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ
            min_length=40,   # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ
            do_sample=False
        )
        return summary[0]['summary_text']
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error during summarization: {e}") 
        return "Error summarizing article."

DB_NAME = 'news_articles.db'
JSON_FILE = 'data/DataNews.json' 

# ----------------------------------------------------
# A. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Web Scraper
# ----------------------------------------------------
def fetch_article_content(url):
    """‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å URL ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å"""
    if not url or url == '#':
        return "No content available."

    try:
        # ‚ö†Ô∏è ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å
        time.sleep(0.5) 
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏•‡∏±‡∏Å ‡πÜ
        article_body = soup.find('article') or soup.find('main') or soup.find('body')
        
        if article_body:
            paragraphs = [p.get_text(strip=True) for p in article_body.find_all('p')]
            # ‡∏£‡∏ß‡∏°‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 50 ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞
            full_text = '\n'.join([p for p in paragraphs if len(p) > 50])
            return full_text if full_text else "Content extraction failed."
        
        return "Content extraction failed (no article/main tag found)."

    except requests.exceptions.RequestException as e:
        # print(f"‚ö†Ô∏è Warning: Failed to fetch URL {url}. Error: {e}")
        return "Content fetch error."
    except Exception as e:
        # print(f"‚ö†Ô∏è Warning: Unexpected error during parsing: {e}")
        return "An unexpected error occurred during parsing."

# ----------------------------------------------------
# B. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ)
# ----------------------------------------------------
# ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå DataNews_source.py

# ... (‡πÇ‡∏Ñ‡πâ‡∏î import ‡πÅ‡∏•‡∏∞‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô get_ai_summary/fetch_article_content) ...

# ----------------------------------------------------
# B. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
# ----------------------------------------------------
def import_data():
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏≤‡∏ò JSON (‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå Importer ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô data/ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô data/ ‡∏î‡πâ‡∏ß‡∏¢)
    if not os.path.exists(JSON_FILE):
        print(f"‚ùå Error: JSON file not found at {JSON_FILE}")
        return
    
    # ‚ö†Ô∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á DB ‡πÉ‡∏´‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° summary
    try:
        os.remove(DB_NAME)
        print(f"üóëÔ∏è Removed old {DB_NAME} file.")
    except FileNotFoundError:
        pass

    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()

    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå summary TEXT
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id TEXT PRIMARY KEY, title TEXT, author TEXT, source_name TEXT,
            image TEXT, url TEXT, publishedAt TEXT, content TEXT, summary TEXT,
            likes INTEGER, comments INTEGER, shares INTEGER, saved BOOLEAN, category TEXT
        )
    """)
    print("‚úÖ Table 'articles' ensured.")

    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            articles = data.get('articles', data) 
            
    except Exception as e:
        print(f"‚ùå Error reading or parsing JSON: {e}")
        return

    # 4. ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏û‡∏£‡πâ‡∏≠‡∏° Scrape ‡πÅ‡∏•‡∏∞ AI Summarization)
    total_articles = len(articles)
    print(f"Starting scraping and import of {total_articles} articles...")
    
    for i, article in enumerate(articles):
        article_url = article.get('url', '#')
        
        # 1. Scrape ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°
        full_content = fetch_article_content(article_url)
        
        # 2. ‚≠ê‚≠ê ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ AI Summarizer ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ ‚≠ê‚≠ê
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Scrape ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏û‡∏≠ (100 ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞)
        if "Content fetch error." in full_content or "Content extraction failed." in full_content or len(full_content) < 100:
            summary_text = "Content too short or fetch failed, cannot summarize."
            print(f"[{i+1}/{total_articles}] ‚ö†Ô∏è Skipping summary for: {article.get('title', 'Untitled')[:30]}...")
        else:
            try:
                # ‚ö° ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                summary_text = get_ai_summary(full_content) 
                print(f"[{i+1}/{total_articles}] Scraped: {article.get('title', 'Untitled')[:30]}... -> Summary length: {len(summary_text)} chars.")
            except Exception as e:
                summary_text = "Error during AI summarization."
                print(f"[{i+1}/{total_articles}] ‚ùå AI Error: {e}")
        
        # 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤/‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤
        article_data = {
            'id': article.get('id') or os.urandom(16).hex(), 
            'title': article.get('title', 'Untitled'),
            'author': article.get('author') or article.get('source', {}).get('name') or 'Unknown',
            'source_name': article.get('source', {}).get('name') or article.get('source') or 'Unknown',
            'image': article.get('urlToImage') or article.get('image') or 'No Image',
            'url': article_url,
            'publishedAt': article.get('publishedAt') or '2023-01-01T00:00:00Z',
            'content': full_content,
            'summary': summary_text, # ‚úÖ ‡πÉ‡∏ä‡πâ summary_text ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å AI
            'likes': article.get('likes', 0),
            'comments': article.get('comments', 0),
            'shares': article.get('shares', 0),
            'saved': 0, 
            'category': article.get('source', {}).get('category') or article.get('category') or 'news'
        }

        try:
            # 4. INSERT INTO articles
            cursor.execute("""
                INSERT INTO articles VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article_data['id'], article_data['title'], article_data['author'], article_data['source_name'],
                article_data['image'], article_data['url'], article_data['publishedAt'], article_data['content'], article_data['summary'],
                article_data['likes'], article_data['comments'], article_data['shares'],
                article_data['saved'], article_data['category']
            ))
        except sqlite3.IntegrityError:
            print(f"‚ö†Ô∏è Skipped duplicate ID: {article_data['id']}")
            
    conn.commit()
    conn.close()
    print(f"‚úÖ Data imported successfully! Total articles: {total_articles} in {DB_NAME}")

if __name__ == '__main__':
    import_data()