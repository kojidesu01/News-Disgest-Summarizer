import requests
from bs4 import BeautifulSoup
import time
import json


JSON_FILE = 'data/DataNews.json'

def fetch_article_content(url):
    """
    ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å URL ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
    """
    if not url or url == '#':
        return "No content available."

    try:
        # ‚≠ê ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏á (Sleep) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÇ‡∏î‡∏¢‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå
        time.sleep(1) # ‡∏´‡∏ô‡πà‡∏ß‡∏á 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°
        
        # ‡∏ï‡∏±‡πâ‡∏á User-Agent ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏•‡∏≠‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏à‡∏£‡∏¥‡∏á
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # Raise exception ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö HTTP error (4xx or 5xx)

        soup = BeautifulSoup(response.content, 'html.parser')
        
        # üí° Web Scraping Tips: 
        # ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å <p> ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏•‡∏±‡∏Å (main, article)
        
        # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏•‡∏±‡∏Å ‡πÜ ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏ä‡πâ
        article_body = soup.find('article') or soup.find('main') or soup.find('body')
        
        if article_body:
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å‡πÅ‡∏ó‡πá‡∏Å <p> ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
            paragraphs = [p.get_text(strip=True) for p in article_body.find_all('p')]
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà captions ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏ô‡∏π
            full_text = '\n'.join([p for p in paragraphs if len(p) > 50])
            
            if full_text:
                return full_text
        
        return "Content extraction failed (generic)."

    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è Warning: Failed to fetch URL {url}. Error: {e}")
        return "Content fetch error."
    except Exception as e:
        return "An unexpected error occurred during parsing."



with open(JSON_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            urls = data.get('url', data) 

for url in urls:
     fetch_article_content(url)

